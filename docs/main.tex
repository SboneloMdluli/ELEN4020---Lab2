%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[10pt,journal]{article}
\input{structure.tex} 
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{float}
\usepackage{multirow}
\title{	
	\normalfont\largesize
	\textbf{\textsc{University of the Witwatersrand, Johannesburg}}\\
	\textsc{School of Electrical and Information Engineering}\\ 
	\vspace{5pt} % Whitespace
	\rule{\linewidth}{0.5pt}\\ % Thin top horizontal rule
	\vspace{10pt} % Whitespace
	{\huge ELEN4020A: Data Intensive Computing: Lab 2}\\ % The assignment title
	\vspace{1pt} % Whitespace
	\rule{\linewidth}{2pt}\\ % Thick bottom horizontal rule
	\vspace{1pt} % Whitespace
}

\author{ Sbonelo Mdluli(1101772), Heemal Ryan(792656), Haroon Rehman(1438756) } 

\date{\large\today}

\begin{document}
\maketitle 

\section{Introduction}
Matrix transposition is a key operation in various scientific and mathematical applications. Matrix transposition converts a M-rows-by-N columns array to a N-rows-by-M columns array. This lab introduces students to parallel programming through the use of Pthread and OpenMP libraries. Matrix transposition of large matrices can be a time and space expensive operation. Parallelism is used to improve the performance of the operation and to improve space requirement matrix transposition is to be done in place. This report will discuss the different matrix transposition algorithms that are developed with their performance for a 2D matrix with varying dimensions. 

\section{Matrix Transpose Algorithms}

The generated matrix is stored using row major ordering in a 1-D array with the exception of the block algorithm which uses 2-D arrays in C++ to store the matrix. This is especially useful for block-oriented transposition as it is easier to conceptualize smaller sub-matrices within a larger matrix. The use of arrays allows for element access in almost constant time resulting in relatively faster run times (compared to using C++ vector), which is the prime focus of this lab.    

\subsection{Basic}

The basic matrix transpose algorithm sections the matrix into a lower and an upper triangle about the principal diagonal. The principal diagonal is is never transposed as it remains unchanged even when transposed. The algorithm uses two nested for loops, the outer and inner for loops specify an element in the upper triangle and the corresponding element in the lower triangle is determined based on the index of the element is the upper matrix.

\subsection{Block}

Initially recursion used to perform the block transpose algorithm however this implementation was later changed as recursion is memory intensive and defeated the purpose of the lab. The algorithm used here entails a twofold process. Firstly, contiguous square sub-matrices of certain length from the main matrix are extracted. These sub-matrices are transposed independently, and placed back into the main matrix. Secondly, after all the sub-matrices are transposed, the main matrix is transposed "block" wise as normal matrix transposition. These blocks are of the same length as the sub-matrices mentioned in part 1. 

\subsection{Diagonal}

The diagonal algorithms uses the basic matrix transpose for its functionality. This algorithm also makes uses of two nested for loops, however the outer for loop starts at a specified diagonal. The diagonal is not specified using the array index but can be accessed numerically with the top left diagonal equal to zero. The transposition only takes place at the diagonal where the row and column intersect. 

\section{Pthreads}

POSIX threads are used to achieve parallelism in shared memory. Pthreads provide the programmer with a lot of freedom and allows one to produce portable multithreded code. 

\subsection{Diagonal-Threading}
 Initially all threads will be assigned to work on specified diagonal corresponding to its thread id. Once a thread has finished transposing it increments a global variable \textit {next\_pos} which is used to communicate the next available diagonal. To avoid race conditions \textit {pthread\_mutex\_lock} is used to only allowed one thread to update this variable at a time. The threads terminate once \textit {next\_pos} is equal to the size of the matrix. Structs were used to better represent the information the threads needed to work properly. The pseudo-code for this method is shown under section A.3 in Appendix A. 

\newline 

\subsection{Block-Oriented}
Here each thread works on a certain "block" within the main matrix corresponding to its thread number. Through this, each thread extracts a sub-matrix (which is a 2-D array as well), transposes it using the basic algorithm and then puts it back into the main matrix. Then, similarly to the diagonal method, a global variable \textit {next\_pos} is used to communicate the next available block. To avoid race conditions \textit {pthread\_mutex\_lock} is used to only allowed one thread to update this variable at a time. The threads terminate once \textit {next\_pos} reaches the last column of the last row block-wise. In the last part, these blocks are transposed using the basic transpose method, however now 2-D arrays are being swapped. This part does not involve any threads unfortunately due to time constraints. The pseudo-code for this method is shown under section A.7 in Appendix A. 

\section{OpenMP}

Open specifications for Multi Processing (OpenMP) offers a high level shared memory parallelism model compared to pthreads. OpenMP is based on the fork-join execution model that is at the beginning of a program the master thread executes and forks whenever there is a parallel block. OpenMP is often easier to use compared to pthreads since the compiler does most of the low level work.

\subsection{Naive}
The naive implementation of the matrix transposition algorithm is heavily based on the basic algorithm. To parallelize the nested for-loops two compiler directives are used namely :
\newline
{\#pragma omp parallel shared(arr) private( i, j) num_threads(NUM\_THREADS) } 
\newline
{\#pragma omp for schedule(static,size)  nowait}
\newline

The first directive parallelizes the nested loops, variable \textit{i}, \textit{j} are made private for each thread to ensure that the threads perform independent transposition. We use schedule because we have a work sharing for-loop. Nowait is used to instruct the thread to continue computing the next transpose.

\subsection{Diagonal-Threading}
The diagonal version of the version in OpenMP shares great similarities with that naive implementation with the exception that scheduling is dynamic and each thread works on a chunk of size one. The scheduling was changed to dynamic as the static one is less efficient and dynamic scheduling allowed a thread to move on to the next task once it finished its execution. The pseudo-code for this method is shown under section A.4 in Appendix A. 

\subsection{Block-Oriented}
The implementation of the block-oriented transposition with OpenMP is almost identical to that of the pthread implementation. The only difference is the way threads are created. Another difference is that the second part of the transposition incorporates multi-threading whereas the pthread method did not. The second part is also almost identical to the naive OpenMP implementation, with a slight change to "block" transposition rather than element transposition. The pseudo-code for this method is shown under section A.6 in Appendix A. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results and Analysis}\
% Please add the following required packages to your document preamble:
Table 5.1 contains the results for the different algorithms discussed in the previous sections. The code for each algorithm are run on computers with the same specifications. Some of these are: Windows 10, Intel Core i7-6700 CPU with 3.41 GHz speed and 4 cores with 8 logical processors.

\begin{table}
\centering
\caption{\label{tab:table-name}Algorithm Benchmark running time}
\begin{tabular}{|c|c|c|c|c|c|c|}

\hline

\multirow{2}{*}{\textbf{$N_{0}=N_{1}$}} & \textbf{Basic} & \multicolumn{2}{c|}{\textbf{Pthreads}} & \multicolumn{3}{c|}{\textbf{OpenMP}} \\ \cline{2-7} 
                                    &                & Diagonal           & Blocked           & Naive       & Diagonal            & Blocked           \\ \hline
\textbf{128}                        & 0.156 ms       & 0.613 ms           & 1.557 ms          & 1.873 ms       & 3.253 ms            & 3.738 ms          \\ \hline
\textbf{1024}                       & 4.471 ms       & 2.165 ms           & 8.325 ms          & 8.554 ms           & 16.84 ms            & 11.48 ms          \\ \hline
\textbf{2048}                       & 37.70 ms       & 8.457 ms           & 27.54 ms          & 45.38 ms             & 76.67 ms            & 30.58 ms           \\ \hline
\textbf{4096}                       & 0.233 s        & 44.62 ms            & 0.104 s          & 0.283 s            & 0.321 s             & 0.105 s           \\ \hline
\end{tabular}
\end{table}


The blocked algorithm was tested using square blocks of length 64. Furthermore, all the multi-threaded algorithms used a constant number of 8 threads. Besides both the block algorithm implementations (pthreads and OpenMP) which were coded in C++, the rest were coded in C. The code is intended to be as scalable as possible through allowance to change the number of threads, the size of the matrix and the length of the blocks for the blocked algorithm. 
\newline

After testing, it is noticed that using larger block sizes increases the speed, however only to a certain degree. It is also observed that the size of the main matrix plays the fundamental role in the speeds, which is expected. However, looking at the results in table 5.1, the use of different algorithms and multi-threading as compared to the basic method do not necessarily increase the speed of computations. But it is also noticeable that as the size of the matrix increases, the use of other algorithms and multi-threading do begin to outperform the basic algorithm. The diagonal pthread implementation is seen to be the fastest for the largest matrix by a order of 10 compared to the others.\\

\section{Conclusion}

A variation of matrix transpose algorithms were developed and parallelized using Pthreads and OpenMp. Based on the results obtained a threaded algorithm does not necessarily mean higher performance there are various factors that should be considered when one parallelizes an algorithm such as data size. The 
threaded algorithms performed better when the size of the matrix was large but poorly for small matrix sizes.

\justify

\newpage
\begin{thebibliography}{9}
\bibitem{Pthreads} 
Pthreads
\\\textit{https://computing.llnl.gov/tutorials/pthreads/}. 

 
\bibitem{OpenMP} 
OpenMP
\\\textit{https://computing.llnl.gov/tutorials/openMP/}. 

 
\bibitem{Jaka's corner} 
OpenMP: For & Scheduling
\\\texttt{http://jakascorner.com/blog/2016/06/omp-for-scheduling.html}
\end{thebibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix
\newpage
\section{Appendix}
\subsection{General Function Used Across Algorithms}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[H]
\SetAlgoLined
\textbf{Function name:} \emph{Swap}\\\vspace{10pt}
Input arguments:\: int* num1, int* num2\\

Initialization\:long int temp = 0\\
     temp = *num\\
    *num1 = *num2\\
    *num2 = temp\\

\caption{Void Swap Function}
\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Naive Approach}
\begin{algorithm}[H]
\SetAlgoLined
\textbf{Function name:} \emph{*CreateMatrix}\\\vspace{10pt}
Input arguments:\:matrixSize\\

Initialization\::\\ 
 int *arr = (int *)calloc(matrixSize * matrixSize, sizeof(int))\\
 val = 1
 
\vspace{10pt} 
\For{i=0  to size-1}{
    \For{j= 0 to size-1}{*(arr + i*size + j) = val++\\
}}

\caption{Void Function to Create and Populate Input Matrix}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[H]
\SetAlgoLined
\textbf{Function name:} \emph{Transpose}\\\vspace{10pt}
Input arguments:\: int* arr, int matrixSize \\

\vspace{10pt} 
\For{i=0  to size-1}{
    \For{j= 0 to size-1}{swap( (arr + i*matrixSize +j), (arr + j*matrixSize +i)\\
}}

\caption{Void Transpose Function}
\end{algorithm}

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Diagonal Approach - PThreads}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Global Variables:} \\
int NumberOfThreads = 8\\
int nextPosition = 1\\
int *arr\\
\vspace{10pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[H]
\SetAlgoLined
\textbf{Function name:} \emph{*CreateMatrix}\\\vspace{10pt}
Initialization\::\\
int *C = (int *)calloc(matrixSize * matrixSize, sizeof(int))\\
val = 1

\vspace{10pt} 
\For{i=0  to size-1}{
    \For{j= 0 to size-1}{ *(C + i*matrixSize + j) = val++
}}
\Return C
\caption{int Function to Create and Populate Input Matrix}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[H]
\SetAlgoLined
\textbf{Function name:} \emph{*Transpose}\\\vspace{10pt}
Initialization\::\\
struct details* local = args\\
pos= 0, n= 0, nextPosition= 1\\

\vspace{10pt} 
\While{pos < matrixSize}{pos = local->index\\
    \For{i=pos  to matrixSize-1}{
        \For{j = pos+1 to matrixSize-1}{swap((arr + pos*matrixSize + j),(arr + j*matrixSize + pos))\\
        }break
    }
    \textbf{pthread\_mutex\_lock(\&lock)}\\
	n = nextPosition++\\
	local->index = n\\
	\textbf{pthread\_mutex\_unlock(\&lock)}\\
    
}
\caption{Void Transpose Function}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[H]
\SetAlgoLined
\textbf{Function name:} \emph{Main Function}\\\vspace{10pt}
Initialization\::\\
matrixSize={128, 1024, 2048 ,4096}\\
struct details args[NumberOfThreads]\\
pthread\_t threads[matrixSize]\\

\vspace{3pt} 
\textbf{Call to Create\::}\\
arr = CreateMatrix(matrixSize)\\

\vspace{10pt} 
\For{i=0  to NumberOfThreads}{
     args[i].index = i\\
	 \textbf{pthread\_create(\&threads[i],NULL, transpose,(void*) \&args)}\\
}

\vspace{10pt} 
\For{i=0  to NumberOfThreads}{
     args[i].index = i\\
	 \textbf{pthread\_join(threads[i],NULL)}\\
}
exit(0)

\caption{Main Function - PThreads}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\vspace{20pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Diagonal Approach - OpenMP}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Global Variables:} \\
int NumberOfThreads = 8\\
\vspace{10pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[H]
\SetAlgoLined
\textbf{Function name:} \emph{*CreateMatrix}\\\vspace{10pt}
Input arguments:\:matrixSize\\
Initialization\::\\
int *arr = (int *)calloc(size * size, sizeof(int))\\
val = 1;

\vspace{10pt} 
\textbf{\#pragma omp for schedule (static, size)}\\
\For{i=0  to size-1}{
    \For{j= 0 to size-1}{*(arr + i*size + j) = val++
}}
\Return arr
\caption{int Function to Create and Populate Input Matrix}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[H]
\SetAlgoLined
\textbf{Function name:} \emph{*Transpose}\\\vspace{10pt}
Input arguments:\:matrixSize, int*arr\\
Initialization\::pos= 0,\\

\vspace{10pt} 
\While{pos < matrixSize}{pos = local->index\\
\textbf{\#pragma omp parallel shared(pos) private( i, j) num-threads(NumberOfThreads}\\ 
    \For{i=pos  to matrixSize-1}{
        \For{j = pos+1 to matrixSize-1}{swap((arr + pos*matrixSize + j),(arr + j*matrixSize + pos))\\
        }break
    }
    \textbf{\#pragma omp critical}{pos++;}

}
\caption{Void Transpose Function}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[H]
\SetAlgoLined
\textbf{Function name:} \emph{Main Function}\\\vspace{10pt}
Initialization\::\\
matrixSize={128, 1024, 2048 ,4096}\\

\vspace{10pt} 
\For{i=0  to 3}{n = *(size +i)\\ 
                int *A = CreateMatrix(n)\\
                transpose(A,n)\\
}
 exit(0)
\caption{Main Function - OpenMP}
\end{algorithm}

\vspace{20pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Commonly Used Functions in Block Algorithm}
\begin{algorithm}[H]
\SetAlgoLined
\textbf{Function name:} \emph{void createMatrix}\\\vspace{10pt}
Initialization\::long int val=1\\

\vspace{10pt} 
\For{i=0  to size\_mat-1}{
    \For{j= 0 to size\_mat-1}{
            MAT [i][j] = val\\
            val++\\
}
}
\caption{Void Function to Create and Populate Input Matrix}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[H]
\SetAlgoLined
\textbf{Function name:} \emph{void getSubMatrix}\\\vspace{10pt}
Input arguments:\: int A, int B\\long int (\&arr)[block\_len][block\_len])\\

\vspace{10pt} 
\For{i=0  to block-length}{
    \For{j= 0 to block\_len}{arr[i][j] = MAT[A+i][B+j];\\
}}
\caption{Void Function to get SubMatrix}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[H]
\SetAlgoLined
\textbf{Function name:} \emph{void setSubMatrix}\\\vspace{10pt}
Input arguments:\: int A, int B\\const long int (&block)[block\_len][block\_len])\\

\vspace{10pt} 
\For{i=0  to block\_len}{
    \For{j= 0 to block\_len}{MAT[A+i][B+j]= block[i][j]\\
}}
\caption{Void Function to set SubMatrix}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[H]
\SetAlgoLined
\textbf{Function name:} \emph{void transpose}\\\vspace{10pt}
Input arguments:\: long int (&arr)[block\_len][block\_len]\\

\vspace{10pt} 
\For{i=0  to size\_mat-1}{
    \For{j= 0 to size\_mat-1}{swap(arr[i][j],arr[j][i])\\
}}
\caption{Void Function to perform normal 2x2 Transformations}
\end{algorithm}

\newpage
\subsection{Block Approach - OpenMP}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Global Variables:} \\

const int block\_len = 2\\ 
const int size\_mat = 4096\\
long int MAT[size\_mat][size\_mat]\\
const int NUM\_THREADS=8\\
int next-pos = NUM\_THREADS\\

\vspace{10pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[H]
\SetAlgoLined
\textbf{Function name:} \emph{void shuffle}\\
\vspace{10pt} 
    \textbf{\#pragma omp parallel num\_threads(NUM\_THREADS)}

\For{i=0  to size\_mat-1}{
    \textbf{\#pragma omp for schedule(static,size\_mat) nowait}\\
    \For{j= 0 to size\_mat-1}{
           getSubMatrix(i,j,block)\\
           getSubMatrix(j,i,block2)\\
           setSubMatrix(i,j,block2)\\
           setSubMatrix(j,i,block)\\
           j+=block\_length\\
}i+=block\_length\\}
\caption{Void Function to Transpose actual bigger blocks}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[H]
\SetAlgoLined
\textbf{Function name:} \emph{void transpose\_blocks}\\\vspace{10pt}
Initialization\::int thread\_counter = 0\\


\vspace{10pt} 
\textbf{\#pragma omp parallel shared (thread\_counter) num\_threads(NUM\_THREADS)}\\
    Initialization\::long local = thread\_counter\\
    thread\_-counter++, int pos = 0\\
    int posY = 0 , int tmp = size\_mat/block\_len\\
    int x = 0 , long int block[block\_len][block\_len]\\
\While{1}{
        pos = (block\_len*local)\\
        x = local/tmp\\
        posY = x*block\_len\\
        \eIf {posY >= size-mat}{break}{\\
        getSubMatrix(posY,pos,block)\\
        transpose(block)\\
        setSubMatrix(posY,pos,block)\\

        \textbf{#pragma omp critical}\\
            local= next\_pos\\
            next\_pos++
	  }
}
\caption{Void Transpose Function: Transposes all inner blocks of matrix}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[H]
\SetAlgoLined
\textbf{Function name:} \emph{Main Function}\\\vspace{10pt}
CreateMatrix()\\
transpose\_blocks()\\
shuffle()\\
\Return(0)\\

\caption{Main Function - OpenMP}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Block Approach - PThreads}
\textbf{Global Variables:} \\
const int block\_len = 2\\
const int size\_mat = 4096\\
long int MAT[size\_mat][size\_mat]\\
const int NUM\_THREADS = 8\\
int next\_pos=NUM\_THREADS\\
pthread\_mutex\_t lock\\
\vspace{10pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[H]
\SetAlgoLined
\textbf{Function name:} \emph{void shuffle}\\\vspace{10pt}
Initialization\::long int block[block\_len][block\_len]\\
      long int block2[block\_len][block\_len];\\
\vspace{10pt} 
\For{i=0  to size\_mat-1}{
    \For{j= 0 to size\_mat-1}{
            getSubMatrix(i,j,block)\\
            getSubMatrix(j,i,block2)\\
            setSubMatrix(i,j,block2)\\
            setSubMatrix(j,i,block)\\
            j+=block\_length\\
}i+=block\_length\\}
\caption{Void Function to Transpose actual bigger blocks}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[H]
\SetAlgoLined
\textbf{Function name:} \emph{*void transpose\_blocks(void* \_ind)}\\\vspace{10pt}
Initialization\::long local\\
    local = (long) \_ind\\
    int pos\\
    int posY = 0 , int tmp = size\_mat/block\_len\\
    int x = 0 , long int block[block\_len] [block\_len]

\vspace{10pt} 

\While{1}{
        pos = (block\_len*local)\\
        x = local/tmp\\
        posY = x*block\_len\\
        \eIf {posY >= size\_mat}{break}{\\
        block = getSubMatrix(posY,pos,block)\\
        transpose(block)\\
        setSubMatrix(posY,pos,block)\\

        \textbf{pthread\_mutex\_lock(\&lock)}\\
            local= next\_pos\\
            next\_pos++\\
        \textbf{pthread\_mutex\_unlock(&lock)}
	  }
}
\caption{Void Transpose Function: Transposes all inner blocks of matrix}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[H]
\SetAlgoLined
\textbf{Function name:} \emph{Main Function}\\\vspace{10pt}
\textbf{Call to Create\::}\\
CreateMatrix()\\
Initialization\::int index[NUM\_THREADS]\\
pthread\_t threads[NUM\_THREADS]

\vspace{10pt} 
\For{i=0  to NUM\_THREADS-1}{
index[i]=i\\
pthread\_create(&threads[i],NULL,transpose\_blocks,(void*)index[i])\\
}
\For{i=0  to NUM\_THREADS-1}{
pthread\_join(threads[i],NULL)\\
}

Shuffle()\\
\Return(0)\\

\caption{Main Function - PThreads}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
